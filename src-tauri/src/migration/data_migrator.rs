// æ•°æ®è¿ç§»å™¨
// è´Ÿè´£ä»Pythonæ•°æ®åº“è¿ç§»æ•°æ®åˆ°Rustæ•°æ®åº“

use crate::crypto::CryptoService;
use crate::database::DatabaseManager;
use crate::models::*;
use anyhow::{Context, Result};
use sqlx::{Row, SqlitePool};
use tracing::{debug, error, info, warn};

/// æ•°æ®è¿ç§»ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Default)]
pub struct MigrationStats {
    pub total_records: i64,
    pub migrated_records: i64,
    pub failed_records: i64,
    pub tables_processed: usize,
    pub errors: Vec<String>,
}

impl MigrationStats {
    pub fn success_rate(&self) -> f64 {
        if self.total_records == 0 {
            100.0
        } else {
            (self.migrated_records as f64 / self.total_records as f64) * 100.0
        }
    }
}

/// æ•°æ®è¿ç§»å™¨
pub struct DataMigrator {
    db_manager: DatabaseManager,
    #[allow(dead_code)]
    crypto_service: CryptoService,
}

impl DataMigrator {
    /// åˆ›å»ºæ–°çš„æ•°æ®è¿ç§»å™¨å®ä¾‹
    pub fn new(db_manager: DatabaseManager, crypto_service: CryptoService) -> Self {
        Self { db_manager, crypto_service }
    }

    /// ä»Pythonæ•°æ®åº“è¿ç§»æ•°æ®
    pub async fn migrate_from_python_db(&self, python_db_path: &str) -> Result<MigrationStats> {
        info!("å¼€å§‹ä»Pythonæ•°æ®åº“è¿ç§»æ•°æ®: {}", python_db_path);

        let mut stats = MigrationStats::default();

        // è¿æ¥åˆ°Pythonæ•°æ®åº“
        let python_pool = self
            .connect_to_python_db(python_db_path)
            .await
            .context("è¿æ¥Pythonæ•°æ®åº“å¤±è´¥")?;

        // æ‰§è¡Œè¡¨ç»“æ„è¿ç§»
        self.migrate_schemas(&python_pool).await.context("è¿ç§»è¡¨ç»“æ„å¤±è´¥")?;

        // è¿ç§»å„ä¸ªè¡¨çš„æ•°æ®
        info!("è¿ç§»è¡¨: claude_providers");
        match self.migrate_claude_providers(&python_pool).await {
            Ok(table_stats) => {
                stats.total_records += table_stats.total_records;
                stats.migrated_records += table_stats.migrated_records;
                stats.failed_records += table_stats.failed_records;
                stats.tables_processed += 1;

                if !table_stats.errors.is_empty() {
                    stats.errors.extend(table_stats.errors);
                }

                info!(
                    "è¡¨ claude_providers è¿ç§»å®Œæˆ: {}/{} æˆåŠŸ",
                    table_stats.migrated_records, table_stats.total_records
                );
            }
            Err(e) => {
                error!("è¿ç§»è¡¨ claude_providers å¤±è´¥: {}", e);
                stats.errors.push(format!("è¡¨ claude_providers è¿ç§»å¤±è´¥: {}", e));
            }
        }

        info!("è¿ç§»è¡¨: codex_providers");
        match self.migrate_codex_providers(&python_pool).await {
            Ok(table_stats) => {
                stats.total_records += table_stats.total_records;
                stats.migrated_records += table_stats.migrated_records;
                stats.failed_records += table_stats.failed_records;
                stats.tables_processed += 1;

                if !table_stats.errors.is_empty() {
                    stats.errors.extend(table_stats.errors);
                }

                info!(
                    "è¡¨ codex_providers è¿ç§»å®Œæˆ: {}/{} æˆåŠŸ",
                    table_stats.migrated_records, table_stats.total_records
                );
            }
            Err(e) => {
                error!("è¿ç§»è¡¨ codex_providers å¤±è´¥: {}", e);
                stats.errors.push(format!("è¡¨ codex_providers è¿ç§»å¤±è´¥: {}", e));
            }
        }

        info!("è¿ç§»è¡¨: agent_guides");
        match self.migrate_agent_guides(&python_pool).await {
            Ok(table_stats) => {
                stats.total_records += table_stats.total_records;
                stats.migrated_records += table_stats.migrated_records;
                stats.failed_records += table_stats.failed_records;
                stats.tables_processed += 1;

                if !table_stats.errors.is_empty() {
                    stats.errors.extend(table_stats.errors);
                }

                info!(
                    "è¡¨ agent_guides è¿ç§»å®Œæˆ: {}/{} æˆåŠŸ",
                    table_stats.migrated_records, table_stats.total_records
                );
            }
            Err(e) => {
                error!("è¿ç§»è¡¨ agent_guides å¤±è´¥: {}", e);
                stats.errors.push(format!("è¡¨ agent_guides è¿ç§»å¤±è´¥: {}", e));
            }
        }

        info!("è¿ç§»è¡¨: mcp_servers");
        match self.migrate_mcp_servers(&python_pool).await {
            Ok(table_stats) => {
                stats.total_records += table_stats.total_records;
                stats.migrated_records += table_stats.migrated_records;
                stats.failed_records += table_stats.failed_records;
                stats.tables_processed += 1;

                if !table_stats.errors.is_empty() {
                    stats.errors.extend(table_stats.errors);
                }

                info!(
                    "è¡¨ mcp_servers è¿ç§»å®Œæˆ: {}/{} æˆåŠŸ",
                    table_stats.migrated_records, table_stats.total_records
                );
            }
            Err(e) => {
                error!("è¿ç§»è¡¨ mcp_servers å¤±è´¥: {}", e);
                stats.errors.push(format!("è¡¨ mcp_servers è¿ç§»å¤±è´¥: {}", e));
            }
        }

        info!("è¿ç§»è¡¨: common_configs");
        match self.migrate_common_configs(&python_pool).await {
            Ok(table_stats) => {
                stats.total_records += table_stats.total_records;
                stats.migrated_records += table_stats.migrated_records;
                stats.failed_records += table_stats.failed_records;
                stats.tables_processed += 1;

                if !table_stats.errors.is_empty() {
                    stats.errors.extend(table_stats.errors);
                }

                info!(
                    "è¡¨ common_configs è¿ç§»å®Œæˆ: {}/{} æˆåŠŸ",
                    table_stats.migrated_records, table_stats.total_records
                );
            }
            Err(e) => {
                error!("è¿ç§»è¡¨ common_configs å¤±è´¥: {}", e);
                stats.errors.push(format!("è¡¨ common_configs è¿ç§»å¤±è´¥: {}", e));
            }
        }

        // å…³é—­Pythonæ•°æ®åº“è¿æ¥
        python_pool.close().await;

        info!(
            "æ•°æ®è¿ç§»å®Œæˆ: æ€»è®¡ {} æ¡è®°å½•ï¼ŒæˆåŠŸ {} æ¡ï¼Œå¤±è´¥ {} æ¡",
            stats.total_records, stats.migrated_records, stats.failed_records
        );
        info!("è¿ç§»æˆåŠŸç‡: {:.2}%", stats.success_rate());

        Ok(stats)
    }

    /// è¿æ¥åˆ°Pythonæ•°æ®åº“
    async fn connect_to_python_db(&self, db_path: &str) -> Result<SqlitePool> {
        let connection_string = format!("sqlite:{}", db_path);
        let pool = SqlitePool::connect(&connection_string).await?;
        Ok(pool)
    }

    /// è¿ç§»è¡¨ç»“æ„
    async fn migrate_schemas(&self, python_pool: &SqlitePool) -> Result<()> {
        info!("æ£€æŸ¥å¹¶è¿ç§»è¡¨ç»“æ„...");

        let tables = vec![
            "claude_providers",
            "codex_providers",
            "agent_guides",
            "mcp_servers",
            "common_configs",
        ];

        for table in tables {
            if !self.table_exists(self.db_manager.pool(), table).await? {
                info!("åˆ›å»ºè¡¨: {}", table);
                self.create_table_from_python(python_pool, table).await?;
            }
        }

        Ok(())
    }

    /// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    async fn table_exists(&self, pool: &SqlitePool, table_name: &str) -> Result<bool> {
        let query = sqlx::query("SELECT name FROM sqlite_master WHERE type='table' AND name=?")
            .bind(table_name);

        let result = query.fetch_optional(pool).await?;
        Ok(result.is_some())
    }

    /// ä»Pythonæ•°æ®åº“ç»“æ„åˆ›å»ºè¡¨
    async fn create_table_from_python(
        &self,
        _python_pool: &SqlitePool,
        table_name: &str,
    ) -> Result<()> {
        let create_sql = match table_name {
            "claude_providers" => {
                r#"
                CREATE TABLE claude_providers (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    url TEXT NOT NULL,
                    token TEXT NOT NULL,
                    max_tokens INTEGER DEFAULT 4096,
                    temperature REAL DEFAULT 0.7,
                    model TEXT DEFAULT 'gpt-4',
                    enabled INTEGER DEFAULT 1,
                    description TEXT,
                    timeout INTEGER DEFAULT 30,
                    retry_count INTEGER DEFAULT 3,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            "#
            }

            "codex_providers" => {
                r#"
                CREATE TABLE codex_providers (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    url TEXT NOT NULL,
                    token TEXT NOT NULL,
                    type TEXT,
                    enabled INTEGER DEFAULT 1,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            "#
            }

            "agent_guides" => {
                r#"
                CREATE TABLE agent_guides (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    description TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            "#
            }

            "mcp_servers" => {
                r#"
                CREATE TABLE mcp_servers (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    url TEXT,
                    command TEXT,
                    args TEXT,
                    enabled INTEGER DEFAULT 1,
                    description TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            "#
            }

            "common_configs" => {
                r#"
                CREATE TABLE common_configs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    key TEXT UNIQUE NOT NULL,
                    value TEXT NOT NULL,
                    type TEXT DEFAULT 'string',
                    description TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            "#
            }

            _ => return Err(anyhow::anyhow!("æœªçŸ¥çš„è¡¨ç±»å‹: {}", table_name)),
        };

        sqlx::query(create_sql).execute(self.db_manager.pool()).await?;
        Ok(())
    }

    /// è¿ç§»Claudeä¾›åº”å•†æ•°æ®
    async fn migrate_claude_providers(&self, python_pool: &SqlitePool) -> Result<MigrationStats> {
        let query = r#"
            SELECT id, name, url, token, timeout, created_at, updated_at
            FROM claude_providers
            ORDER BY id
        "#;

        let rows = sqlx::query(query).fetch_all(python_pool).await?;
        let mut stats = MigrationStats { total_records: rows.len() as i64, ..Default::default() };

        for row in rows {
            let provider = CreateClaudeProviderRequest {
                name: row.get("name"),
                url: row.get("url"),
                token: row.get("token"), // ä¿æŒåŠ å¯†çŠ¶æ€
                timeout: row.try_get("timeout").ok(),
                auto_update: None,
                r#type: None,
                opus_model: None,
                sonnet_model: None,
                haiku_model: None,
            };

            match self.create_claude_provider(&provider).await {
                Ok(_) => {
                    stats.migrated_records += 1;
                    debug!("Claudeä¾›åº”å•† {} è¿ç§»æˆåŠŸ", row.get::<i64, _>("id"));
                }
                Err(e) => {
                    stats.failed_records += 1;
                    let error_msg =
                        format!("Claudeä¾›åº”å•† {} è¿ç§»å¤±è´¥: {}", row.get::<i64, _>("id"), e);
                    warn!("{}", error_msg);
                    stats.errors.push(error_msg);
                }
            }
        }

        Ok(stats)
    }

    /// è¿ç§»Codexä¾›åº”å•†æ•°æ®
    async fn migrate_codex_providers(&self, python_pool: &SqlitePool) -> Result<MigrationStats> {
        let query = r#"
            SELECT id, name, url, token, type, created_at, updated_at
            FROM codex_providers
            ORDER BY id
        "#;

        let rows = sqlx::query(query).fetch_all(python_pool).await?;
        let mut stats = MigrationStats { total_records: rows.len() as i64, ..Default::default() };

        for row in rows {
            let provider = CreateCodexProviderRequest {
                name: row.get("name"),
                url: row.get("url"),
                token: row.get("token"), // ä¿æŒåŠ å¯†çŠ¶æ€
                r#type: row.try_get("type").ok(),
            };

            match self.create_codex_provider(&provider).await {
                Ok(_) => {
                    stats.migrated_records += 1;
                    debug!("Codexä¾›åº”å•† {} è¿ç§»æˆåŠŸ", row.get::<i64, _>("id"));
                }
                Err(e) => {
                    stats.failed_records += 1;
                    let error_msg =
                        format!("Codexä¾›åº”å•† {} è¿ç§»å¤±è´¥: {}", row.get::<i64, _>("id"), e);
                    warn!("{}", error_msg);
                    stats.errors.push(error_msg);
                }
            }
        }

        Ok(stats)
    }

    /// è¿ç§»AgentæŒ‡å¯¼æ•°æ®
    async fn migrate_agent_guides(&self, python_pool: &SqlitePool) -> Result<MigrationStats> {
        let query = r#"
            SELECT id, name, type, text, created_at, updated_at
            FROM agent_guides
            ORDER BY id
        "#;

        let rows = sqlx::query(query).fetch_all(python_pool).await?;
        let mut stats = MigrationStats { total_records: rows.len() as i64, ..Default::default() };

        for row in rows {
            let guide = CreateAgentGuideRequest {
                name: row.get("name"),
                r#type: row.try_get("type").unwrap_or_else(|_| "default".to_string()),
                text: row.try_get("text").unwrap_or_default(),
            };

            match self.create_agent_guide(&guide).await {
                Ok(_) => {
                    stats.migrated_records += 1;
                    debug!("AgentæŒ‡å¯¼ {} è¿ç§»æˆåŠŸ", row.get::<i64, _>("id"));
                }
                Err(e) => {
                    stats.failed_records += 1;
                    let error_msg =
                        format!("AgentæŒ‡å¯¼ {} è¿ç§»å¤±è´¥: {}", row.get::<i64, _>("id"), e);
                    warn!("{}", error_msg);
                    stats.errors.push(error_msg);
                }
            }
        }

        Ok(stats)
    }

    /// è¿ç§»MCPæœåŠ¡å™¨æ•°æ®
    async fn migrate_mcp_servers(&self, python_pool: &SqlitePool) -> Result<MigrationStats> {
        let query = r#"
            SELECT id, name, type, timeout, command, args, env, created_at, updated_at
            FROM mcp_servers
            ORDER BY id
        "#;

        let rows = sqlx::query(query).fetch_all(python_pool).await?;
        let mut stats = MigrationStats { total_records: rows.len() as i64, ..Default::default() };

        for row in rows {
            // è§£æ argsï¼ˆå‡è®¾å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
            let args_str: String = row.try_get("args").unwrap_or_default();
            let args: Vec<String> = serde_json::from_str(&args_str).unwrap_or_default();

            // è§£æ envï¼ˆå‡è®¾å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
            let env_str: Option<String> = row.try_get("env").ok();
            let env: Option<std::collections::HashMap<String, String>> =
                env_str.and_then(|s| serde_json::from_str(&s).ok());

            let server = CreateMcpServerRequest {
                name: row.get("name"),
                r#type: row.try_get("type").ok(),
                timeout: row.try_get("timeout").ok(),
                command: row.get("command"),
                args,
                env,
            };

            match self.create_mcp_server(&server).await {
                Ok(_) => {
                    stats.migrated_records += 1;
                    debug!("MCPæœåŠ¡å™¨ {} è¿ç§»æˆåŠŸ", row.get::<i64, _>("id"));
                }
                Err(e) => {
                    stats.failed_records += 1;
                    let error_msg =
                        format!("MCPæœåŠ¡å™¨ {} è¿ç§»å¤±è´¥: {}", row.get::<i64, _>("id"), e);
                    warn!("{}", error_msg);
                    stats.errors.push(error_msg);
                }
            }
        }

        Ok(stats)
    }

    /// è¿ç§»é€šç”¨é…ç½®æ•°æ®
    async fn migrate_common_configs(&self, python_pool: &SqlitePool) -> Result<MigrationStats> {
        let query = r#"
            SELECT id, key, value, description, category, is_active, created_at, updated_at
            FROM common_configs
            ORDER BY id
        "#;

        let rows = sqlx::query(query).fetch_all(python_pool).await?;
        let mut stats = MigrationStats { total_records: rows.len() as i64, ..Default::default() };

        for row in rows {
            let config = CreateCommonConfigRequest {
                key: row.get("key"),
                value: row.get("value"),
                description: row.try_get("description").ok(),
                category: row.try_get("category").ok(),
                is_active: row.try_get("is_active").ok(),
            };

            match self.create_common_config(&config).await {
                Ok(_) => {
                    stats.migrated_records += 1;
                    debug!("é€šç”¨é…ç½® {} è¿ç§»æˆåŠŸ", row.get::<i64, _>("id"));
                }
                Err(e) => {
                    stats.failed_records += 1;
                    let error_msg = format!("é€šç”¨é…ç½® {} è¿ç§»å¤±è´¥: {}", row.get::<i64, _>("id"), e);
                    warn!("{}", error_msg);
                    stats.errors.push(error_msg);
                }
            }
        }

        Ok(stats)
    }

    /// åˆ›å»ºClaudeä¾›åº”å•†è®°å½•
    async fn create_claude_provider(&self, request: &CreateClaudeProviderRequest) -> Result<i64> {
        let id = sqlx::query(
            r#"
            INSERT INTO claude_providers (name, url, token, timeout, auto_update, type, opus_model, sonnet_model, haiku_model)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#
        )
        .bind(&request.name)
        .bind(&request.url)
        .bind(&request.token)
        .bind(request.timeout)
        .bind(request.auto_update)
        .bind(&request.r#type)
        .bind(&request.opus_model)
        .bind(&request.sonnet_model)
        .bind(&request.haiku_model)
        .execute(self.db_manager.pool())
        .await?;

        let id = id.last_insert_rowid();
        Ok(id)
    }

    /// åˆ›å»ºCodexä¾›åº”å•†è®°å½•
    async fn create_codex_provider(&self, request: &CreateCodexProviderRequest) -> Result<i64> {
        let id = sqlx::query(
            r#"
            INSERT INTO codex_providers (name, url, token, type)
            VALUES (?, ?, ?, ?)
            "#,
        )
        .bind(&request.name)
        .bind(&request.url)
        .bind(&request.token)
        .bind(&request.r#type)
        .execute(self.db_manager.pool())
        .await?;

        let id = id.last_insert_rowid();
        Ok(id)
    }

    /// åˆ›å»ºAgentæŒ‡å¯¼è®°å½•
    async fn create_agent_guide(&self, request: &CreateAgentGuideRequest) -> Result<i64> {
        let id = sqlx::query(
            r#"
            INSERT INTO agent_guides (name, type, text)
            VALUES (?, ?, ?)
            "#,
        )
        .bind(&request.name)
        .bind(&request.r#type)
        .bind(&request.text)
        .execute(self.db_manager.pool())
        .await?;

        let id = id.last_insert_rowid();
        Ok(id)
    }

    /// åˆ›å»ºMCPæœåŠ¡å™¨è®°å½•
    async fn create_mcp_server(&self, request: &CreateMcpServerRequest) -> Result<i64> {
        // å°† args åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²
        let args_json = serde_json::to_string(&request.args)?;

        // å°† env åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²
        let env_json = request.env.as_ref().map(serde_json::to_string).transpose()?;

        let id = sqlx::query(
            r#"
            INSERT INTO mcp_servers (name, type, timeout, command, args, env)
            VALUES (?, ?, ?, ?, ?, ?)
            "#,
        )
        .bind(&request.name)
        .bind(&request.r#type)
        .bind(request.timeout)
        .bind(&request.command)
        .bind(&args_json)
        .bind(&env_json)
        .execute(self.db_manager.pool())
        .await?;

        let id = id.last_insert_rowid();
        Ok(id)
    }

    /// åˆ›å»ºé€šç”¨é…ç½®è®°å½•
    async fn create_common_config(&self, request: &CreateCommonConfigRequest) -> Result<i64> {
        let id = sqlx::query(
            r#"
            INSERT INTO common_configs (key, value, description, category, is_active)
            VALUES (?, ?, ?, ?, ?)
            "#,
        )
        .bind(&request.key)
        .bind(&request.value)
        .bind(&request.description)
        .bind(&request.category)
        .bind(request.is_active)
        .execute(self.db_manager.pool())
        .await?;

        let id = id.last_insert_rowid();
        Ok(id)
    }

    /// ç”Ÿæˆè¿ç§»æŠ¥å‘Š
    pub async fn generate_migration_report(&self, stats: &MigrationStats) -> String {
        let mut report = String::new();

        report.push_str("# æ•°æ®è¿ç§»æŠ¥å‘Š\n\n");
        report.push_str(&format!(
            "ç”Ÿæˆæ—¶é—´: {}\n\n",
            chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
        ));

        report.push_str("## è¿ç§»ç»Ÿè®¡\n\n");
        report.push_str(&format!("- æ€»è®°å½•æ•°: {}\n", stats.total_records));
        report.push_str(&format!("- æˆåŠŸè¿ç§»: {}\n", stats.migrated_records));
        report.push_str(&format!("- å¤±è´¥è®°å½•: {}\n", stats.failed_records));
        report.push_str(&format!("- å¤„ç†è¡¨æ•°: {}\n", stats.tables_processed));
        report.push_str(&format!("- æˆåŠŸç‡: {:.2}%\n\n", stats.success_rate()));

        if !stats.errors.is_empty() {
            report.push_str("## é”™è¯¯è¯¦æƒ…\n\n");
            for (i, error) in stats.errors.iter().enumerate() {
                report.push_str(&format!("{}. {}\n", i + 1, error));
            }
        }

        // æ€»ä½“è¯„ä¼°
        report.push_str("## è¿ç§»è¯„ä¼°\n\n");
        if stats.success_rate() >= 99.0 {
            report.push_str("ğŸ‰ **è¿ç§»æˆåŠŸï¼** æ•°æ®è¿ç§»å®Œå…¨æˆåŠŸï¼Œæ— æ•°æ®ä¸¢å¤±ã€‚\n");
        } else if stats.success_rate() >= 95.0 {
            report.push_str("âœ… **è¿ç§»åŸºæœ¬æˆåŠŸ**ï¼Œæœ‰å°‘é‡æ•°æ®é—®é¢˜éœ€è¦å¤„ç†ã€‚\n");
        } else if stats.success_rate() >= 80.0 {
            report.push_str("âš ï¸ **è¿ç§»éƒ¨åˆ†æˆåŠŸ**ï¼Œæœ‰ä¸€äº›æ•°æ®é—®é¢˜éœ€è¦é‡ç‚¹å¤„ç†ã€‚\n");
        } else {
            report.push_str("âŒ **è¿ç§»å¤±è´¥**ï¼Œå­˜åœ¨ä¸¥é‡çš„æ•°æ®é—®é¢˜ï¼Œéœ€è¦é‡æ–°è¿ç§»ã€‚\n");
        }

        report
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_migration_stats_success_rate() {
        let mut stats = MigrationStats::default();
        stats.total_records = 100;
        stats.migrated_records = 95;
        stats.failed_records = 5;

        assert_eq!(stats.success_rate(), 95.0);

        // æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        stats.total_records = 0;
        assert_eq!(stats.success_rate(), 100.0);
    }

    #[tokio::test]
    async fn test_data_migrator_creation() {
        let temp_dir = tempdir().unwrap();
        let db_path = temp_dir.path().join("test.db");

        let db_config = crate::database::DatabaseConfig {
            url: format!("sqlite:{}", db_path.display()),
            max_connections: 5,
            min_connections: 1,
            connect_timeout: std::time::Duration::from_secs(5),
            idle_timeout: std::time::Duration::from_secs(60),
            max_lifetime: std::time::Duration::from_secs(300),
        };

        let db_manager = DatabaseManager::new(db_config).await.unwrap();
        let crypto_service = CryptoService::new("test_key_for_migration").unwrap();

        let migrator = DataMigrator::new(db_manager, crypto_service);

        // æµ‹è¯•åˆ›å»ºæˆåŠŸ
        assert!(!migrator.db_manager.pool().is_closed());
    }
}
